# PCA_feature_extraction_and_dimensionality-reduction_ML
Applications of Artificial Intelligence, Machine Learning Classification models and statistical effect of the PCA algorithm.

Feature Extraction and Dimensionality Reduction with Principal Component Analysis (PCA) and Comparison Accuracy 6 Machine Learning Classification Models: before-after PCA.
  
It is observed that reducing the size with PCA, that is, reducing the number of variables, has a positive effect on the success score of some machine learning classification models. It is possible to produce more effective and faster solutions by taking a small amount of data loss. Reducing dimensions with PCA will provide us with great convenience, especially in studies related to Big Data.  
  
 
## Project Steps
  
Step 1:   Collect Data: UCI Parkinson's Disease Classification Data Set

Step 2:   Eigendecomposition - Eigenvalues, Eigenvectors and Eigenspace

Step 3:   Primary Component Selection

Step 4:   Projection New Feature Space

Step 5:   Principal Component Analysis (PCA)

Step 6:   Comparison Accurancy 6 Machine Learning Models : before-after PCA

1. Model : Logistic Regression

2. Model : Support Vector Machines (SVM)

3. Model : Decision Tree Classifier

4. Model : KNN(k-nearest neighbors algorithm)

5. Model : Random Forest Classifier

6. Model: Gaussian Naive Bayes




Collect Data: UCI Parkinson's Disease Classification Data Set
https://archive.ics.uci.edu/ml/datasets/Parkinson%27s+Disease+Classification



**Note:** *I chose the (n_components)top 6 components with the highest variance. anyone can give a different number. It is an optional choice. Decide to process only 6 of the 754 features with the highest variance. It reduces the size very much and enables fast processing and only the most effective features will be processed.
PCA enabled only 6 variables to be processed instead of 754 variables.*
